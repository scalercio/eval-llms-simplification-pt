# -- fix path --
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).resolve().parent.parent))
# -- end fix path --
import json
from easse.sari import corpus_sari
import warnings
warnings.filterwarnings("ignore")
from src.utils import remove_indexes, create_unique_vector, write_lines

"""
Implements the selection of the best and worst simplification sentences generated by GPT using the SARI metric.
Creates an file with the best sentences and another with the worst sentences. Also, identifies the most influential source files in the
selection, and calculates the final SARI for 'museu' and 'porsimplessent' dataset with best and worst simplification sentences generated by GPT.
"""

def calculate_sentence_counts(selected_indices, num_files, types, label, dataset):
    counts = [0] * num_files
    for idx in selected_indices:
        counts[idx] += 1

    print(f"{dataset}: Number of sentences added in {label}_sentences_{dataset}.txt per source file:")
    for type_idx, tipo in enumerate(types):
        for i in range(3):
            file_index = type_idx * 3 + i
            print(f"{tipo} {i+1}: {counts[file_index]} sentences")

def final_sari_for_each_dataset_with_best_or_worst_sentences(ref_seq, src_seq, selected_sentences, label, dataset):
    final_sari = corpus_sari(
        orig_sents=src_seq,
        sys_sents=selected_sentences,
        refs_sents=[ref_seq]
    )
    print(f"Final SARI score for {dataset} ({label}): {final_sari}")

if __name__ == '__main__':
    types = ['sintática', 'anáfora', 'ordem', 'redundante_lexical']
    datasets = ['public_simple_language']
    
    remove_duplicates = True
    if remove_duplicates and 'public_simple_language' in datasets:
        with open('data/public_simple_language_2/test.simple', 'r') as f1, open('data/public_simple_language_2/test.complex', 'r') as f2:
            ref_seq = f1.read()
            ref_seq = ref_seq.splitlines()
            src_seq = f2.read()
            src_seq = src_seq.splitlines()
            
            _ , indexes = create_unique_vector(src_seq, ref_seq)
            print(len(indexes))
            src_seq = remove_indexes(src_seq, indexes)
            ref_seq = remove_indexes(ref_seq, indexes)
            #write_lines(src_seq, f'data/public_simple_language_2/test.complex_filtered')
            #write_lines(ref_seq, f'data/public_simple_language_2/test.simple_filtered')

    for dataset in datasets:
        for is_best in (True, False):

            with open(f'data/{dataset}/test.simple', 'r') as f1, open(f'data/{dataset}/test.complex', 'r') as f2:
                ref_seq = f1.readlines()
                src_seq = f2.readlines()

            selected_sentences = []
            selected_indices = []
            num_files = len(types) * 3
            label = 'best' if is_best else 'worst'

            for idx, src_sentence in enumerate(src_seq):
                ref_sentences = [ref_seq[idx]]
                simplified_sentences = []

                for tipo_one_shot in types:
                    for i in ['7', '77', '777']:
                        simplified_file = f'simplified_outputs/{dataset}/gpt-4o-mini/simplified_{tipo_one_shot}_{i}.json'
                        with open(simplified_file, 'r') as f3:
                            data = json.load(f3)

                        simplified_sentences.append(data[idx]['simplified'])

                best_or_worst_sari = -1 if is_best else 1000
                best_or_worst_sentence = ""
                best_or_worst_index = -1

                for i, sys_sentence in enumerate(simplified_sentences):
                    sari_score = corpus_sari(
                        orig_sents=[src_sentence],
                        sys_sents=[sys_sentence],
                        refs_sents=[ref_sentences]
                    )

                    if (is_best and sari_score > best_or_worst_sari) or (not is_best and sari_score < best_or_worst_sari):
                        best_or_worst_sari = sari_score
                        best_or_worst_sentence = sys_sentence
                        best_or_worst_index = i

                selected_sentences.append(best_or_worst_sentence)
                selected_indices.append(best_or_worst_index)

            if remove_duplicates and 'public_simple_language' in datasets:
                selected_indices = remove_indexes(selected_indices, indexes)
                selected_sentences = remove_indexes(selected_sentences, indexes)
                src_seq = remove_indexes(src_seq, indexes)
                ref_seq = remove_indexes(ref_seq, indexes)
                assert len(selected_sentences) == len(selected_indices)
                assert len(selected_sentences) == len(src_seq)
                assert len(selected_sentences) == len(ref_seq)
                print(f"Sentences after filtering: {len(selected_sentences)}")
                
            output_file = f'data/{dataset}_2/{label}_sentences_{dataset}.txt'
            #with open(output_file, 'w') as f_out:
            #    for sentence in selected_sentences:
            #        f_out.write(f"{sentence}\n")
            print(f"{label} sentences for {dataset} saved in: {output_file}")

            calculate_sentence_counts(selected_indices, num_files, types, label, dataset)

            final_sari_for_each_dataset_with_best_or_worst_sentences(ref_seq, src_seq, selected_sentences, label, dataset)
